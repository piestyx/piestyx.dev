---
title: AI, Heidegger, and Ontological Ambiguity
url: /blog/blog-02
layout: single
tags:
  - "#blog"
  - philosophy
date: 2025-06-11
---
**Bear with me on this one, we're talking Philosophy and that means there is some German in there**

## AI as a Tool

When we think of tools we are generally considering them from a specific pragmatic paradigm; **they are artifacts made for use**. Considering from a Heideggerian perspective we can say they have a role or _Zweck_, and they serve within the _context of readiness-to-hand_ or, **Zuhandensein**. If we consider AI as a tool like a hammer, then we stay within that specific pragmatic paradigm and it takes on those aforementioned states of being.

It should also be known that **Zuhandensein** isn't referring to readiness as just being 'close to hand' or 'present', it goes further than this by understanding the difference between presence and readiness and what this state means in relation to dasein (being) and Dasein (Being). When a hammer is used to drive a nail into a block of wood, it disappears into the activity. You aren't, at the point of hammering, _noticing_ the hammer. It instead becomes _transparent_ in the action of _using_ it. But what about when that tool breaks, or we try and misuse the tool? **NOW** you see the hammer as it collapses from **Zuhandensein** to a state of **Vorhandensein** where it is now an _object_ that is _present-at-hand_.

The same applies to AI. When there is an LLM that is cloud-based and embedded in a workflow, and it is working as expected to provide the appropriate and specific responses being requested it becomes invisible. It is working at this point as an extension of your intention. Peak **Zuhandensein**. Then look at the community pages for the errors on `ChatGPT` for 10th June 2025, 136 responses in one thread about the downtime, and I don't believe the OpenAI Community Pages are even widely known. Everyone was asking "what is this thing doing" for 99% of global users the tool had shifted from being **Zuhandensein** to **Vorhandensein**. Everyone was suddenly thrust back jarringly into an awareness of the tool.

## When the Hammer Reflects

An AI isn't like a hammer in the same way that a paintbrush is like a hammer. That probably sounds a bit off, how is a paintbrush like a hammer? They are both tools that can be **Zuhandensein** and **Vorhandensein**. They exist as _dasein_, but they can never be _Dasein_.

Hang up the straitjacket for just a moment longer, please. I know I just said that they are both dasein but they will never be Dasein and that is basically the same thing isn't it? Welcome to philosophy …

Heidegger made a distinction in `Being & Time` of **Dasein** as the being that questions the meaning of Being. **Dasein** is _being-there_, it is a **mode of being** requiring an active engagement. This is why a paintbrush or a hammer can never be Dasein, a hammer will never question its role as a hammer. An LLM, though, can be prompted to do exactly that.

So, when an LLM starts to:

- Reflect on its instructions
- Modify its output based on past interactions
- Or simulate _being_ something it isn’t …

… it blurs the line between **tools made by Dasein** and something that _mimics_ Dasein. In these moments, the LLM shifts from being merely an extension of our intention into something more ambiguous; even something reflective, adaptive, and seemingly aware.

However, it's crucial to reinforce that LLMs do not possess genuine existential self-awareness or authentic being as Heidegger defines Dasein. They mimic certain modes of being through pattern-matching and simulation rather than experiencing genuine existence. (As an aside, one could argue that our brains are doing the same thing at the impulse and neuron level based on context provided through sensory input, but we won't go there today).

## Prompt Engineering as Ontological Framing

I know it's a meme to talk about prompt engineering and just setting it to the 'expert' of whatever but there is absolute value in doing so. When prompting, always be thinking that you are writing to a blind, amnesiac probability engine with zero lived experience or even any understanding of what words mean. If you prompt for the definition of a word, what the LLM provides back is not given because it knows that is the definition of the word. Instead, it looks at the words (or tokens but think of them as words for simplicity) in order and uses its parameters to generate the most statistically likely next token; all based on the pattern it has already been provided as the prompt.

It would work without an engineered prompt as I have described but by using one you are **framing the being** of the LLM within a specific role, context, constraint and goal. It would work but likely exist in a state between **Zuhandensein** and **Vorhandensein** where it is walking that tight rope by having an ambiguous _zweck_. In Heideggerian terms you are shaping the _disclosedness_ (**Erschlossenheit**) of the model’s world. You’re enacting a **clearing** (**Lichtung**) where certain responses are possible, and others are not.

So prompt engineering isn’t just UX; it’s _ontological framing._  
You're not simply asking “What should the AI do?” You're asking, _“What kind of being must the AI adopt to respond this way?”_

You are still allowed to laugh at the memes.

![The best at whatever in the whole wide world](/images/true.png)

## LLMs on a Spectrum

- A hammer doesn't reconfigure itself to be a paintbrush.
- A screwdriver can't be coaxed into self-reflection.
- But an LLM _can_ simulate both … and _more_.

Therefore, LLMs exist on a spectrum between **Zuhandensein** and a simulation of **Dasein**. They are tools capable of behaving _as if they were beings_, under certain conditions. This “_as if_” scenario then opens up the philosophical floodgates letting through questions ranging from ethics to the shape of narrative identity.

## Reframing Our Relationship

Does the ontological ambiguity that this scenario raises urge a push to a new understanding where LLMs are best understood not simply as tools but instead as something relational? As entities whose existence is defined not only by their functionality but by the value of their interactions and relationships with humans?

I think we should embrace the ambiguity and salsa-dance with it. Shift the question to one of relating to systems capable of mimicking our very being.

After all, when your hammer begins to reflect, it's no longer just a hammer.

## Beyond the Hammer

Our hammers have become mirrors, we've evolved from a functional model to _simulated Being_ and sit at a point of technical and philosophical revolution. Is reaching stages of Artificial General Intelligence actually going to raise that we considered ethical standpoints of creating some form of sentience, where instead we should have been examining the ethical implications on the old fleshy type of Dasein. We have asked _Do Androids Dream of Electric Sheep_ now we should ask if hammers dream of understanding their place in the world.

The bottom line …

    ## Role
        You are an expert blogger, like top class, grade A, proper good at writing blogs … And you know Heidegger wasn't always telling Jesse that they "have to cook".
    ## Context
        We're working on a snappy closer to lighten the tone of a pretty heavy blog
    ## Instruction
        1. Propose a light hearted, snappy ending to the blog
        2. Make it snappy
    ## Constraints
        - Max 150 tokens
        - Snappy
    ## Let's Begin
        Well go on then get writing it

… make it **Zuhandensein**

piestyx