<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning | piestyx.dev</title>
<meta name=keywords content="#blog,#hnx-m,#helixnet"><meta name=description content="A discussion on the reasoning, hypothesis and structure of the HNX-M HelixNet Architecture…"><meta name=author content="piestyx"><link rel=canonical href=https://piestyx.dev/blog/hnx-01/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://piestyx.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://piestyx.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://piestyx.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://piestyx.dev/apple-touch-icon.png><link rel=mask-icon href=https://piestyx.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://piestyx.dev/blog/hnx-01/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://piestyx.dev/css/custom.css><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><meta property="og:url" content="https://piestyx.dev/blog/hnx-01/"><meta property="og:site_name" content="piestyx.dev"><meta property="og:title" content="HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning"><meta property="og:description" content="A discussion on the reasoning, hypothesis and structure of the HNX-M HelixNet Architecture…"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-08-15T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-15T00:00:00+00:00"><meta property="article:tag" content="#Blog"><meta property="article:tag" content="#Hnx-M"><meta property="article:tag" content="#Helixnet"><meta name=twitter:card content="summary"><meta name=twitter:title content="HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning"><meta name=twitter:description content="A discussion on the reasoning, hypothesis and structure of the HNX-M HelixNet Architecture…"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://piestyx.dev/blog/"},{"@type":"ListItem","position":2,"name":"HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning","item":"https://piestyx.dev/blog/hnx-01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning","name":"HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning","description":"A discussion on the reasoning, hypothesis and structure of the HNX-M HelixNet Architecture…","keywords":["#blog","#hnx-m","#helixnet"],"articleBody":"Abstract HNX-M is a dual-strand, entropy-gated recurrent architecture inspired by the double-helix structure of DNA. It maintains forward and backward causal processing paths connected by sparsely activated memory “rungs” that store salient past states for direct retrieval. This post outlines the biological and computational rationale, details the architecture, and reports initial reinforcement learning experiments in the MiniGrid PPO benchmark. Results show parity in stability and convergence with a tuned CNN baseline, suggesting HNX-M can be deployed as a drop-in feature extractor without short-horizon penalty.\nIntroduction When we refer to “memory” in machine learning, it often means a sequence of stored activations, replay buffers, or key–value caches designed for recent recall. In biology, memory takes a fundamentally different form: it is embedded in the structure of the organism itself, refined over evolutionary time. DNA is one of the most compact, robust, and efficient information systems known, capable of storing, compressing, and transmitting the complete set of instructions required to construct and maintain a living system.\nFrom a computational perspective:\nStorage: Four nucleotides (A, T, G, C) encode an entire biological blueprint using a minimal alphabet. Compression: Evolutionary selection has produced a highly compressed, redundancy-tolerant encoding over billions of years. Transfer: DNA supports loss-resistant duplication and transmission across generations. DNA can therefore be considered a structural memory substrate — a physical manifestation of past selection pressures and adaptive success. Each genome is, in effect, a long-term, read-write archive of what has worked in the past.\nThis work explores whether the principles underlying DNA’s double-helix — specifically, dual complementary strands linked by discrete anchors — can inform the design of artificial neural memory systems. The central hypothesis is:\nA dual-strand neural architecture, with explicit, sparse inter-strand “rungs” acting as memory anchors, can enable selective, high-efficiency recall of relevant historical states, improving stability and reasoning over extended time horizons without scaling parameter count.\nThe resulting model, HNX-M, implements this architecture as two recurrent processing streams — a forward causal path and a backward causal path — connected by trainable anchor gates. The forward strand processes incoming sequences, while the backward strand can directly retrieve and integrate past context by traversing anchor connections, bypassing the need for stepwise backtracking.\nThe following sections summarise the design rationale, testing methodology, and initial findings.\nHypothesis Primary Hypothesis: A dual-strand recurrent architecture, in which forward and backward causal processing paths are connected via sparsely activated, trainable “anchor rungs,” will support targeted retrieval of long-range temporal dependencies with lower computational cost than stepwise recurrent traversal.\nSecondary Hypothesis: By coupling rung placement to learned salience signals (state deltas and entropy) and constraining memory writes through sparsity-regularised gates, the architecture will exhibit improved stability and long-horizon reasoning in reinforcement learning environments without parameter count scaling.\nMethods Got it — here’s your blog draft with the Architecture section expanded to integrate the structural performance mapping and biological analogies, and the Methods → Results → Discussion → Conclusion flow updated to centre the MiniGrid PPO benchmark as the first proper baseline test.\nArchitecture HNX-M implements two independent temporal processing strands:\nForward Strand — Processes input sequences chronologically using a causal convolutional projection layer with torsion-based positional modulation (project_with_torsion_split). Backward Strand — Processes features in reverse temporal order, scaled by an entropy-dependent gate (BackwardScaler) to suppress high-uncertainty retrievals. The strands are coupled via Gated Memory Rungs, which:\nCompute rung salience from state delta (L2 change in hidden state) and entropy of forward-strand activations. Write selectively into a LearnableDecayMemory module with per-slot decay parameters. Allow backward-strand access to salient past states without iterating over all intermediate timesteps. Figure 1: Input sequences are processed in parallel by a forward and a backward strand, with shared gated memory “rungs” enabling selective recall\nHNX-M v4 Structural Features and Performance Mapping Structural Feature Biological Analogy Primary Functional Role Observed Outcome in Early Tests Dual Strand Processing DNA double helix / bidirectional axons Maintains forward and backward temporal pathways for sensing \u0026 recall Handles asymmetric temporal cues; supports stability Memory Ladder Synaptic weight traces / long-term memory Long-term storage with gradual decay for selective retention Stable access to past information Gated Memory Injection Neurotransmitter gating at dendrites Controls when stored context influences current processing Prevents overwriting; integrates smoothly Entropy-Gated Backward Scaling Attention focus under cognitive load Filters retrieval based on information certainty Reduces noise from irrelevant recalls Asymmetric Slot Projection Functional lateralisation of brain hemispheres Separates writing and reading representations Enables diverse focus strategies per strand Per-Slot Write Strengths Hebbian learning with local adaptation Adjusts how strongly each memory slot is updated Improves retention consistency Positional Biasing via Temporal Offsets Circadian / neural oscillation entrainment Aligns internal processing to temporal patterns Reduces mid-training instability Torsion-Based Signal Gating Protein folding via torsional control Sharpens incoming representations before processing Improves convergence smoothness Adaptive Fusion Weights Hormonal modulation of brain regions Dynamically balances emphasis between memory, current input, retrospection Maintains performance across different task phases Emergent Rung Signal Calcium spike / dendritic activation burst Highlights moments of significant change in state Acts as a logging signal for potential memory updates Where This Fits in the Research Landscape A recent study by Aguilera et al. (2025) introduced curved neural networks; models that alter the geometry of their statistical manifold to incorporate higher-order interactions across all orders while avoiding combinatorial parameter explosion. Their work shows that structural deformations in the underlying space can produce explosive phase transitions, hysteresis, and measurable increases in memory capacity and retrieval robustness.\nHNX-M shares the structural emphasis, but its design principle differs. Curved networks deform the statistical manifold of a recurrent network; HNX-M, by contrast, introduces a dual-strand temporal architecture with sparsely gated “memory rungs” inspired by the physical coupling of DNA strands. Rather than manifold curvature, the mechanism here is explicit path separation and targeted re-linking for selective temporal recall.\nThe convergence between the two lines of work is notable: both point to geometry and structure, not just scale as levers for improving memory efficiency and stability. This emerging theme suggests that architectural interventions at the structural/topological level can meaningfully shift a network’s learning dynamics and long-horizon reasoning ability.\nTraining Setup Baseline Objective to confirm no degradation in short-horizon RL\nMiniGrid PPO Benchmark\nEnvironment: MiniGrid-GoToObject-8x8-N2-v0 (image-only)\nPolicy: CnnPolicy (baseline) vs HNXMFeatures (HNX-M)\nFramework: Stable-Baselines3 PPO\nTimesteps: 100,000 (4 parallel envs)\nHardware: RX 7900 XTX (ROCm), Ryzen 7 7800X3D\nResults Metric Final (CNN) Final (HNX-M) Mean (CNN) Mean (HNX-M) Loss 0.0503 0.0505 0.0168 0.0244 Value Loss 0.0979 0.1010 0.0749 0.0743 Policy Grad. Loss -0.0056 -0.0016 -0.0140 -0.0111 Entropy Loss -1.3369 -1.3532 -1.6617 -1.6426 Explained Var. 0.0894 0.0211 0.1629 0.1340 The two models trained stably and converged to similar performance in this short-horizon task, with no significant degradation in loss, entropy, or explained variance for HNX-M.\nAvailability The results are published on GitHub at piestyx available as the raw TensorBoard logs from MiniGrid.\nDiscussion Viability: This first standardised PPO benchmark shows HNX-M training stably alongside a tuned CNN baseline, despite its more complex internal structure. No tuning: The HNX-M run used default PPO settings and unoptimised gating parameters; performance parity at this stage is encouraging. Architectural intent: The selective recall and dual-strand processing are designed for tasks with longer-term dependencies than MiniGrid provides — future tests will target those domains. Limitations: Single environment, short horizon, and absence of hyperparameter sweeps mean these results are preliminary. Conclusion HNX-M can operate as a drop-in feature extractor in standard RL setups without loss of stability or performance. The next phase will:\nTarget longer-horizon environments where its recall mechanisms matter more. Introduce adaptive gating and salience thresholds. Explore hybrid models combining HNX-M’s temporal recall with attention mechanisms. If these features deliver in the domains they’re built for, HNX-M could be a template for bio-inspired, efficiency-first architectures that prioritise selective rather than exhaustive memory.\n","wordCount":"1287","inLanguage":"en","datePublished":"2025-08-15T00:00:00Z","dateModified":"2025-08-15T00:00:00Z","author":{"@type":"Person","name":"piestyx"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://piestyx.dev/blog/hnx-01/"},"publisher":{"@type":"Organization","name":"piestyx.dev","logo":{"@type":"ImageObject","url":"https://piestyx.dev/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://piestyx.dev/ accesskey=h title="piestyx.dev (Alt + H)">piestyx.dev</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://piestyx.dev/packages/ title=Packages><span>Packages</span></a></li><li><a href=https://piestyx.dev/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://piestyx.dev/project_echo/ title=ECHO><span>ECHO</span></a></li><li><a href=https://piestyx.dev/playground/ title=Playground><span>Playground</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">HNX-M: A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning</h1><div class=post-meta><span title='2025-08-15 00:00:00 +0000 UTC'>August 15, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;piestyx</div></header><div class=post-content><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><p>HNX-M is a dual-strand, entropy-gated recurrent architecture inspired by the double-helix structure of DNA. It maintains forward and backward causal processing paths connected by sparsely activated memory “rungs” that store salient past states for direct retrieval. This post outlines the biological and computational rationale, details the architecture, and reports initial reinforcement learning experiments in the MiniGrid PPO benchmark. Results show parity in stability and convergence with a tuned CNN baseline, suggesting HNX-M can be deployed as a drop-in feature extractor without short-horizon penalty.</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>When we refer to “memory” in machine learning, it often means a sequence of stored activations, replay buffers, or key–value caches designed for recent recall. In biology, memory takes a fundamentally different form: it is embedded in the structure of the organism itself, refined over evolutionary time. DNA is one of the most compact, robust, and efficient information systems known, capable of storing, compressing, and transmitting the complete set of instructions required to construct and maintain a living system.</p><p>From a computational perspective:</p><ul><li><strong>Storage:</strong> Four nucleotides (A, T, G, C) encode an entire biological blueprint using a minimal alphabet.</li><li><strong>Compression:</strong> Evolutionary selection has produced a highly compressed, redundancy-tolerant encoding over billions of years.</li><li><strong>Transfer:</strong> DNA supports loss-resistant duplication and transmission across generations.</li></ul><p>DNA can therefore be considered a <em>structural memory substrate</em> — a physical manifestation of past selection pressures and adaptive success. Each genome is, in effect, a long-term, read-write archive of what has worked in the past.</p><p>This work explores whether the principles underlying DNA’s double-helix — specifically, <strong>dual complementary strands linked by discrete anchors</strong> — can inform the design of artificial neural memory systems. The central hypothesis is:</p><blockquote><p>A dual-strand neural architecture, with explicit, sparse inter-strand “rungs” acting as memory anchors, can enable selective, high-efficiency recall of relevant historical states, improving stability and reasoning over extended time horizons without scaling parameter count.</p></blockquote><p>The resulting model, <strong>HNX-M</strong>, implements this architecture as two recurrent processing streams — a forward causal path and a backward causal path — connected by trainable anchor gates. The forward strand processes incoming sequences, while the backward strand can directly retrieve and integrate past context by traversing anchor connections, bypassing the need for stepwise backtracking.</p><p>The following sections summarise the design rationale, testing methodology, and initial findings.</p><hr><h2 id=hypothesis>Hypothesis<a hidden class=anchor aria-hidden=true href=#hypothesis>#</a></h2><blockquote><p><strong>Primary Hypothesis:</strong> A dual-strand recurrent architecture, in which forward and backward causal processing paths are connected via sparsely activated, trainable “anchor rungs,” will support targeted retrieval of long-range temporal dependencies with lower computational cost than stepwise recurrent traversal.</p><p><strong>Secondary Hypothesis:</strong> By coupling rung placement to learned salience signals (state deltas and entropy) and constraining memory writes through sparsity-regularised gates, the architecture will exhibit improved stability and long-horizon reasoning in reinforcement learning environments without parameter count scaling.</p></blockquote><hr><h2 id=methods>Methods<a hidden class=anchor aria-hidden=true href=#methods>#</a></h2><p>Got it — here’s your blog draft with the <strong>Architecture</strong> section expanded to integrate the structural performance mapping and biological analogies, and the <strong>Methods → Results → Discussion → Conclusion</strong> flow updated to centre the MiniGrid PPO benchmark as the first proper baseline test.</p><hr><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>HNX-M implements two independent temporal processing strands:</p><ol><li><strong>Forward Strand</strong> — Processes input sequences chronologically using a causal convolutional projection layer with torsion-based positional modulation (<code>project_with_torsion_split</code>).</li><li><strong>Backward Strand</strong> — Processes features in reverse temporal order, scaled by an <strong>entropy-dependent gate</strong> (<code>BackwardScaler</code>) to suppress high-uncertainty retrievals.</li></ol><p>The strands are coupled via <strong>Gated Memory Rungs</strong>, which:</p><ul><li>Compute <em>rung salience</em> from <strong>state delta</strong> (L2 change in hidden state) and <strong>entropy</strong> of forward-strand activations.</li><li>Write selectively into a <strong>LearnableDecayMemory</strong> module with per-slot decay parameters.</li><li>Allow backward-strand access to salient past states without iterating over all intermediate timesteps.</li></ul><p><img alt="Figure 1 Dual-Strand Architecture with Memory Ladder. Input sequences are processed in parallel by a forward and a backward strand, with shared gated memory “rungs” enabling selective recall" loading=lazy src=/images/HNX-M-Flat.svg></p><p><em>Figure 1</em>: Input sequences are processed in parallel by a forward and a backward strand, with shared gated memory “rungs” enabling selective recall</p><hr><h3 id=hnx-m-v4-structural-features-and-performance-mapping>HNX-M v4 Structural Features and Performance Mapping<a hidden class=anchor aria-hidden=true href=#hnx-m-v4-structural-features-and-performance-mapping>#</a></h3><table><thead><tr><th>Structural Feature</th><th>Biological Analogy</th><th>Primary Functional Role</th><th>Observed Outcome in Early Tests</th></tr></thead><tbody><tr><td><strong>Dual Strand Processing</strong></td><td>DNA double helix / bidirectional axons</td><td>Maintains forward and backward temporal pathways for sensing & recall</td><td>Handles asymmetric temporal cues; supports stability</td></tr><tr><td><strong>Memory Ladder</strong></td><td>Synaptic weight traces / long-term memory</td><td>Long-term storage with gradual decay for selective retention</td><td>Stable access to past information</td></tr><tr><td><strong>Gated Memory Injection</strong></td><td>Neurotransmitter gating at dendrites</td><td>Controls when stored context influences current processing</td><td>Prevents overwriting; integrates smoothly</td></tr><tr><td><strong>Entropy-Gated Backward Scaling</strong></td><td>Attention focus under cognitive load</td><td>Filters retrieval based on information certainty</td><td>Reduces noise from irrelevant recalls</td></tr><tr><td><strong>Asymmetric Slot Projection</strong></td><td>Functional lateralisation of brain hemispheres</td><td>Separates writing and reading representations</td><td>Enables diverse focus strategies per strand</td></tr><tr><td><strong>Per-Slot Write Strengths</strong></td><td>Hebbian learning with local adaptation</td><td>Adjusts how strongly each memory slot is updated</td><td>Improves retention consistency</td></tr><tr><td><strong>Positional Biasing via Temporal Offsets</strong></td><td>Circadian / neural oscillation entrainment</td><td>Aligns internal processing to temporal patterns</td><td>Reduces mid-training instability</td></tr><tr><td><strong>Torsion-Based Signal Gating</strong></td><td>Protein folding via torsional control</td><td>Sharpens incoming representations before processing</td><td>Improves convergence smoothness</td></tr><tr><td><strong>Adaptive Fusion Weights</strong></td><td>Hormonal modulation of brain regions</td><td>Dynamically balances emphasis between memory, current input, retrospection</td><td>Maintains performance across different task phases</td></tr><tr><td><strong>Emergent Rung Signal</strong></td><td>Calcium spike / dendritic activation burst</td><td>Highlights moments of significant change in state</td><td>Acts as a logging signal for potential memory updates</td></tr></tbody></table><hr><h3 id=where-this-fits-in-the-research-landscape>Where This Fits in the Research Landscape<a hidden class=anchor aria-hidden=true href=#where-this-fits-in-the-research-landscape>#</a></h3><p>A recent study by <a href=https://www.nature.com/articles/s41467-025-61475-w.pdf>Aguilera et al. (2025)</a> introduced curved neural networks; models that alter the geometry of their statistical manifold to incorporate higher-order interactions across all orders while avoiding combinatorial parameter explosion. Their work shows that structural deformations in the underlying space can produce explosive phase transitions, hysteresis, and measurable increases in memory capacity and retrieval robustness.</p><p>HNX-M shares the structural emphasis, but its design principle differs. Curved networks deform the statistical manifold of a recurrent network; HNX-M, by contrast, introduces a dual-strand temporal architecture with sparsely gated “memory rungs” inspired by the physical coupling of DNA strands. Rather than manifold curvature, the mechanism here is explicit path separation and targeted re-linking for selective temporal recall.</p><p>The convergence between the two lines of work is notable: both point to geometry and structure, not just scale as levers for improving memory efficiency and stability. This emerging theme suggests that architectural interventions at the structural/topological level can meaningfully shift a network’s learning dynamics and long-horizon reasoning ability.</p><hr><h3 id=training-setup>Training Setup<a hidden class=anchor aria-hidden=true href=#training-setup>#</a></h3><p><strong>Baseline Objective</strong> to confirm no degradation in short-horizon RL</p><p><strong>MiniGrid PPO Benchmark</strong><br>Environment: <code>MiniGrid-GoToObject-8x8-N2-v0</code> (image-only)<br>Policy: <code>CnnPolicy</code> (baseline) vs <code>HNXMFeatures</code> (HNX-M)<br>Framework: Stable-Baselines3 PPO<br>Timesteps: 100,000 (4 parallel envs)<br>Hardware: RX 7900 XTX (ROCm), Ryzen 7 7800X3D</p><hr><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><table><thead><tr><th>Metric</th><th>Final (CNN)</th><th>Final (HNX-M)</th><th>Mean (CNN)</th><th>Mean (HNX-M)</th></tr></thead><tbody><tr><td><strong>Loss</strong></td><td>0.0503</td><td>0.0505</td><td>0.0168</td><td>0.0244</td></tr><tr><td><strong>Value Loss</strong></td><td>0.0979</td><td>0.1010</td><td>0.0749</td><td>0.0743</td></tr><tr><td><strong>Policy Grad. Loss</strong></td><td>-0.0056</td><td>-0.0016</td><td>-0.0140</td><td>-0.0111</td></tr><tr><td><strong>Entropy Loss</strong></td><td>-1.3369</td><td>-1.3532</td><td>-1.6617</td><td>-1.6426</td></tr><tr><td><strong>Explained Var.</strong></td><td>0.0894</td><td>0.0211</td><td>0.1629</td><td>0.1340</td></tr></tbody></table><p>The two models trained stably and converged to <strong>similar performance</strong> in this short-horizon task, with no significant degradation in loss, entropy, or explained variance for HNX-M.</p><h3 id=availability>Availability<a hidden class=anchor aria-hidden=true href=#availability>#</a></h3><p>The results are published on GitHub at <a href=https://github.com/piestyx/hnx-m>piestyx</a> available as the raw <code>TensorBoard</code> logs from <code>MiniGrid</code>.</p><hr><h2 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h2><ul><li><strong>Viability:</strong> This first standardised PPO benchmark shows HNX-M training stably alongside a tuned CNN baseline, despite its more complex internal structure.</li><li><strong>No tuning:</strong> The HNX-M run used default PPO settings and unoptimised gating parameters; performance parity at this stage is encouraging.</li><li><strong>Architectural intent:</strong> The selective recall and dual-strand processing are designed for tasks with longer-term dependencies than MiniGrid provides — future tests will target those domains.</li><li><strong>Limitations:</strong> Single environment, short horizon, and absence of hyperparameter sweeps mean these results are preliminary.</li></ul><hr><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>HNX-M can operate as a drop-in feature extractor in standard RL setups without loss of stability or performance. The next phase will:</p><ol><li>Target <strong>longer-horizon environments</strong> where its recall mechanisms matter more.</li><li>Introduce <strong>adaptive gating and salience thresholds</strong>.</li><li>Explore <strong>hybrid models</strong> combining HNX-M’s temporal recall with attention mechanisms.</li></ol><p>If these features deliver in the domains they’re built for, HNX-M could be a template for <strong>bio-inspired, efficiency-first architectures</strong> that prioritise <em>selective</em> rather than <em>exhaustive</em> memory.</p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://piestyx.dev/tags/%23blog/>#Blog</a></li><li><a href=https://piestyx.dev/tags/%23hnx-m/>#Hnx-M</a></li><li><a href=https://piestyx.dev/tags/%23helixnet/>#Helixnet</a></li></ul><nav class=paginav><a class=next href=https://piestyx.dev/blog/changelog-02/><span class=title>Next »</span><br><span>Updates</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://piestyx.dev/>piestyx.dev</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>