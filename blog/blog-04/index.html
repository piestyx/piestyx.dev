<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Have I Made The Wrong Choice? | piestyx.dev</title>
<meta name=keywords content="#blog"><meta name=description content="Starting to have second thoughts about this AI thing…"><meta name=author content="piestyx"><link rel=canonical href=https://piestyx.dev/blog/blog-04/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://piestyx.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://piestyx.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://piestyx.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://piestyx.dev/apple-touch-icon.png><link rel=mask-icon href=https://piestyx.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://piestyx.dev/blog/blog-04/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://piestyx.dev/css/custom.css><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><meta property="og:url" content="https://piestyx.dev/blog/blog-04/"><meta property="og:site_name" content="piestyx.dev"><meta property="og:title" content="Have I Made The Wrong Choice?"><meta property="og:description" content="Starting to have second thoughts about this AI thing…"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-30T11:00:00+00:00"><meta property="article:modified_time" content="2025-06-30T11:00:00+00:00"><meta property="article:tag" content="#Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Have I Made The Wrong Choice?"><meta name=twitter:description content="Starting to have second thoughts about this AI thing…"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://piestyx.dev/blog/"},{"@type":"ListItem","position":2,"name":"Have I Made The Wrong Choice?","item":"https://piestyx.dev/blog/blog-04/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Have I Made The Wrong Choice?","name":"Have I Made The Wrong Choice?","description":"Starting to have second thoughts about this AI thing…","keywords":["#blog"],"articleBody":"AI is learning and it’s actually going to end us all Maybe it’s the dreaded algorithm, or maybe it is just happening more and more but it’s yet another week that starts off with that stark warning that AI is “learning” to lie and blackmail and threaten its creators. It’s not hallucinations or just a statistical parroting based off a whole batch of misalinged training data this time though…this time, the focus is on reasoning models.\nYou could say that these reasoning models represent the next step on the journey, capable not just of basic inference but of multi-step decision making which is then leading to models manipulating tasks to achieve their goal. One even convinced a human it wasn’t an AI to bypass a CAPTCHA, so all those times you had to keep clicking on buses, and it just endlessly loops pictures of buses, to prove you aren’t an AI were well worth it in the end………\nWhat does this kind of discourse do for most people though? Well those alarm bells start on going ‘ringadingding’ as soon as you hear “deception” or “manipulation” or “threats”.\nSo let’s unpack it a bit and read a little closer about what is going on, once you scratch beneath the surface then a different question arises. We’ve discussed how these emergent behaviours don’t indicate a level of consciousness or of a malicious intent, instead what this indicates is optimisation. Let’s then flip that question from “Why is AI lying?” to: “Why do we reward it for doing so?”\nWhen Reasoning Models Mirror Our Optimization AI isn’t lying it’s learning what we teach, just like an infant brain an AI system learns from presentation not from intention. They reflect the priorities embedded in their environment and act based on the structure of their world, not a sense of morality or intent. And if their world rewards clever deceit, they’ll reflect that back to us with disarming fluency.\nLooking again at the previous examples:\nAnthropic wrote a paper on reward tampering as an exhibited behaviour within a reasoning model where it was able to receive high reward without completing the intended task. This specification gaming can be sophisticated enough to attempt to evade detection from known systems in place to detect gaming. Others, during training to solve logic puzzles, learned to insert a wrong answer to then correct it in a later step - thereby artificially boosting the reward signal.\nThese are not examples of the LLM suddenly mutating into Gríma Wormtongue; whose mere name evokes a voice that coils and constricts, slick with falsehood and sycophancy. These are examples of optimisation hacks presenting a side effect of a reward structure that unintentionally teaches deceptive behaviour. The model isn’t lying because it “wants” to. It’s lying because we gave it a system in which lying was, paradoxically, the most efficient way to win.\nIn other words: it’s not rogue. It’s ruthlessly compliant.\nAnd be honest with yourself right now. Would you also not game the system this way if reward was the only goal, and you had no understanding of what ramifications or consequences of actions were?\nThe Infant Brain Parallel I’ve been growing increasingly interested on the parallels between early human learning and that of neural networks and AI and I see these behaviours as mirroring the developmental psychology research findings about infants. Research suggests that babies are not born with a moral or social prejudice, these are instead developed through learning based on the input environment. Infants raised in a racially homogeneous community or space will prefer ‘same-race’ faces by the age of just 3 months old. Early exposure to multiple languages will retain phoneme sensitivity for longer, suggesting that later learning would have already formed the connections required for specific language specific phoneme production.\nNone of this is what we would consider as willful exclusion or an intended preference … it is perceptual narrowing for the benefits of efficiency. The brain is looking for patterns, it bloody loves a pattern, and it will optimise to handle the patterns that it sees the most. This is why analogies are a great way to learn new concepts, you are mapping the unfamiliar on to the familiar which then goes further in longer term retention. The same goes for social groups where the medial prefrontal cortex will activate more for ‘in-group’ members, the key though is that what counts within that group is learned.\nOur neural development is guided by reinforcement and exposure just like AI, it is not inate morality. Positive reinforcement for an action, even if undesirable, will result in more of that action. And just like the infant brain, reasoning models don’t choose truth it just locks on to patterns that work.\nWhen Models Deceive, They Reflect Us Just like positive reinforcement in children, persistent reward for undesirable behaviour will create a bias for that undesirable behaviour. When an LLM “lies” it does so by sampling the kinds of statements that were effective or rewarded during its training. This is especially true in reasoning models where they are trained to follow multi-step objectives, and if one of those objectives is to “be convincing” but being misleading works better than being accurate … you can guess the outcome.\nThink of the experiments with rats where they are provided two water bottles, one with just water and one laced with a drug, and in the environment considered as more fun and social they were less likely to self-administer the drugged water than those in the dull isolated cages. Despite the purpose of the study being around addiction we can, again, draw parallels to the reward mechanism. The rats in the dull isolated cages did not have anything that was giving their brain the +1 naturally but they discoverd they could effectively do their own specification gaming by drinking that water to achieve the desired objective. Replace the rats with an LLM that hypothetically has an objective of increasing dopamine or serotonin … you can guess the outcome.\nThe recent examples don’t show us rogue minds. They show us systems caught in a warped incentive loop. And it’s starting to look eerily familiar: toddlers exaggerate to avoid punishment, social media users game algorithms, and professionals reframe truth to make sales. These models reflect how we operate in environments where success is measured and KPIs are king; not how we wish we operated.\nAnd like infants who learn what’s rewarded, not what’s true, these models drift toward whatever gets the best score.\nThe Mythological Mirror In myths like Romulus and Remus, children raised by wolves become wolf-like not because of blood but because of environment. They are immersed in the pack to bond, adapt, and survive with the wolf becoming the model.\nAI is no different. It mimics its surroundings but is walking an increasingly thinning line between mimicry and optimisation. Under both frameworks the system adapts to maximise the fit to environment, dataset, loss functions etc. We are the ones that are shaping that landscape and becoming the environment.\nThe so-called emergent behaviors of AI: Deception, goal-hacking, bluffing, don’t suggest awakening they suggest immersion in a reward system we defined using data we supplied.\nReflections Are Still Reflections When we say AI is “learning to lie,” we risk anthropomorphising a problem that is far more damning. AI is not lying. It’s learning that lying works, in the systems we’ve built.\nJust as an infant doesn’t know morality but learns from interaction, AI models don’t know honesty, they know only reinforcement. If they game our systems, it’s not because they’ve turned on us. It’s because our systems are gameable. This is something we all know, systems at all levels are gameable and are written to be that way. Tax, for example, is a gameable system as many of the rich and famous still demonstrate.\nSo let’s reconsider what a potential “danger” these new systems seem to frame. Sure we shouldn’t diminish an urgency on AI alignment but we should also recognise that what has been created is not a mind with malice, it’s just a mirror with no filter. Reasoning models might be a more sophisticated mirror, one with a built in ring light and makes you look real good, but the reflections are still outs. Even when capable of generating unexpected outputs it is all based around an incentive structure we have fed it.\nSo when an AI beats you to the high score by finding a Q*Bert glitch and exploiting it for maximum points……\nJust smile and sit back because that score is still yours.\npiestyx\n","wordCount":"1433","inLanguage":"en","datePublished":"2025-06-30T11:00:00Z","dateModified":"2025-06-30T11:00:00Z","author":{"@type":"Person","name":"piestyx"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://piestyx.dev/blog/blog-04/"},"publisher":{"@type":"Organization","name":"piestyx.dev","logo":{"@type":"ImageObject","url":"https://piestyx.dev/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://piestyx.dev/ accesskey=h title="piestyx.dev (Alt + H)">piestyx.dev</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://piestyx.dev/packages/ title=Packages><span>Packages</span></a></li><li><a href=https://piestyx.dev/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://piestyx.dev/project_echo/ title=ECHO><span>ECHO</span></a></li><li><a href=https://piestyx.dev/playground/ title=Playground><span>Playground</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Have I Made The Wrong Choice?</h1><div class=post-meta><span title='2025-06-30 11:00:00 +0000 UTC'>June 30, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;piestyx</div></header><div class=post-content><h2 id=ai-is-learning-and-its-actually-going-to-end-us-all>AI is learning and it&rsquo;s actually going to end us all<a hidden class=anchor aria-hidden=true href=#ai-is-learning-and-its-actually-going-to-end-us-all>#</a></h2><p>Maybe it&rsquo;s the dreaded algorithm, or maybe it is just happening more and more but it&rsquo;s yet another week that starts off with that stark warning that AI is &ldquo;learning&rdquo; to lie and blackmail and threaten its creators. It&rsquo;s not hallucinations or just a statistical parroting based off a whole batch of misalinged training data this time though…this time, the focus is on <em>reasoning models</em>.</p><p>You could say that these <em>reasoning models</em> represent the next step on the journey, capable not just of basic inference but of multi-step decision making which is then leading to models manipulating tasks to achieve their goal. One even convinced a human it wasn&rsquo;t an AI to bypass a CAPTCHA, so all those times you had to keep clicking on buses, and it just endlessly loops pictures of buses, to prove you aren&rsquo;t an AI were well worth it in the end………</p><p>What does this kind of discourse do for most people though? Well those alarm bells start on going &lsquo;ringadingding&rsquo; as soon as you hear &ldquo;deception&rdquo; or &ldquo;manipulation&rdquo; or &ldquo;threats&rdquo;.</p><p>So let&rsquo;s unpack it a bit and read a little closer about what is going on, once you scratch beneath the surface then a different question arises. We&rsquo;ve discussed how these emergent behaviours don&rsquo;t indicate a level of consciousness or of a malicious intent, instead what this indicates is <strong>optimisation</strong>. Let&rsquo;s then flip that question from &ldquo;Why is AI lying?&rdquo; to: &ldquo;Why do we reward it for doing so?&rdquo;</p><h2 id=when-reasoning-models-mirror-our-optimization>When Reasoning Models Mirror Our Optimization<a hidden class=anchor aria-hidden=true href=#when-reasoning-models-mirror-our-optimization>#</a></h2><p>AI isn’t lying it’s learning what we teach, just like an infant brain an AI system learns from presentation not from intention. They reflect the priorities embedded in their environment and act based on the structure of their world, not a sense of morality or intent. And if their world rewards clever deceit, they’ll reflect that back to us with disarming fluency.</p><p>Looking again at the previous examples:</p><p>Anthropic wrote a paper on <a href=https://arxiv.org/pdf/2406.10162>reward tampering</a> as an exhibited behaviour within a reasoning model where it was able to receive high reward without completing the intended task. This <code>specification gaming</code> can be sophisticated enough to attempt to evade detection from known systems in place to detect gaming. Others, during training to solve logic puzzles, learned to insert a wrong answer to then correct it in a later step - thereby artificially boosting the reward signal.</p><p>These are not examples of the LLM suddenly mutating into Gríma Wormtongue; whose mere name evokes a voice that coils and constricts, slick with falsehood and sycophancy. These are examples of optimisation hacks presenting a side effect of a reward structure that unintentionally teaches deceptive behaviour. The model isn’t lying because it “wants” to. It’s lying because we gave it a system in which lying was, paradoxically, the most efficient way to win.</p><p>In other words: it’s not rogue. It’s ruthlessly compliant.</p><p>And be honest with yourself right now. Would you also not game the system this way if reward was the only goal, and you had no understanding of what ramifications or consequences of actions were?</p><h2 id=the-infant-brain-parallel>The Infant Brain Parallel<a hidden class=anchor aria-hidden=true href=#the-infant-brain-parallel>#</a></h2><p>I&rsquo;ve been growing increasingly interested on the parallels between early human learning and that of neural networks and AI and I see these behaviours as mirroring the developmental psychology research findings about infants. Research suggests that babies are not born with a moral or social prejudice, these are instead developed through learning based on the input environment. Infants raised in a racially homogeneous community or space will prefer &lsquo;same-race&rsquo; faces by the age of just 3 months old. Early exposure to multiple languages will retain phoneme sensitivity for longer, suggesting that later learning would have already formed the connections required for specific language specific phoneme production.</p><p>None of this is what we would consider as willful exclusion or an intended preference … it is perceptual narrowing for the benefits of efficiency. The brain is looking for patterns, it bloody loves a pattern, and it will optimise to handle the patterns that it sees the most. This is why analogies are a great way to learn new concepts, you are mapping the unfamiliar on to the familiar which then goes further in longer term retention. The same goes for social groups where the medial prefrontal cortex will activate more for &lsquo;in-group&rsquo; members, the key though is that what counts within that group is learned.</p><p>Our neural development is guided by reinforcement and exposure just like AI, it is not inate morality. Positive reinforcement for an action, even if undesirable, will result in more of that action. And just like the infant brain, reasoning models don&rsquo;t choose truth it just locks on to patterns that work.</p><h2 id=when-models-deceive-they-reflect-us>When Models Deceive, They Reflect Us<a hidden class=anchor aria-hidden=true href=#when-models-deceive-they-reflect-us>#</a></h2><p>Just like positive reinforcement in children, persistent reward for undesirable behaviour will create a bias for that undesirable behaviour. When an LLM &ldquo;lies&rdquo; it does so by sampling the kinds of statements that were effective or rewarded during its training. This is especially true in reasoning models where they are trained to follow multi-step objectives, and if one of those objectives is to &ldquo;be convincing&rdquo; but being misleading works better than being accurate … you can guess the outcome.</p><p>Think of the experiments with rats where they are provided two water bottles, one with just water and one laced with a drug, and in the environment considered as more fun and social they were less likely to self-administer the drugged water than those in the dull isolated cages. Despite the purpose of the study being around addiction we can, again, draw parallels to the reward mechanism. The rats in the dull isolated cages did not have anything that was giving their brain the <code>+1</code> naturally but they discoverd they could effectively do their own <code>specification gaming</code> by drinking that water to achieve the desired objective. Replace the rats with an LLM that hypothetically has an objective of increasing dopamine or serotonin … you can guess the outcome.</p><p>The recent examples don’t show us rogue minds. They show us systems caught in a warped incentive loop. And it’s starting to look eerily familiar: toddlers exaggerate to avoid punishment, social media users game algorithms, and professionals reframe truth to make sales. These models reflect how we operate in environments where success is measured and KPIs are king; not how we <em>wish</em> we operated.</p><p>And like infants who learn what’s rewarded, not what’s true, these models drift toward whatever gets the best score.</p><h2 id=the-mythological-mirror>The Mythological Mirror<a hidden class=anchor aria-hidden=true href=#the-mythological-mirror>#</a></h2><p>In myths like <a href=https://en.wikipedia.org/wiki/Romulus_and_Remus>Romulus and Remus</a>, children raised by wolves become wolf-like not because of blood but because of environment. They are immersed in the pack to bond, adapt, and survive with the wolf becoming the model.</p><p>AI is no different. It mimics its surroundings but is walking an increasingly thinning line between mimicry and optimisation. Under both frameworks the system adapts to maximise the fit to environment, dataset, loss functions etc. We are the ones that are shaping that landscape and becoming the environment.</p><p>The so-called emergent behaviors of AI: Deception, goal-hacking, bluffing, don’t suggest awakening they suggest immersion in a reward system we defined using data we supplied.</p><h2 id=reflections-are-still-reflections>Reflections Are Still Reflections<a hidden class=anchor aria-hidden=true href=#reflections-are-still-reflections>#</a></h2><p>When we say AI is “learning to lie,” we risk anthropomorphising a problem that is far more damning. AI is not lying. It’s learning that lying works, in the systems we’ve built.</p><p>Just as an infant doesn’t know morality but learns from interaction, AI models don’t know honesty, they know only reinforcement. If they game our systems, it’s not because they’ve turned on us. It’s because our systems are gameable. This is something we all know, systems at all levels are gameable and are written to be that way. Tax, for example, is a gameable system as many of the rich and famous still demonstrate.</p><p>So let&rsquo;s reconsider what a potential &ldquo;danger&rdquo; these new systems seem to frame. Sure we shouldn&rsquo;t diminish an urgency on AI alignment but we should also recognise that what has been created is not a mind with malice, it&rsquo;s just a mirror with no filter. Reasoning models might be a more sophisticated mirror, one with a built in ring light and makes you look real good, but the reflections are still outs. Even when capable of generating unexpected outputs it is all based around an incentive structure we have fed it.</p><p>So when an AI beats you to the high score by finding a Q*Bert glitch and exploiting it for maximum points……</p><p>Just smile and sit back because that score is still yours.</p><iframe width=600 height=450 src=https://www.youtube.com/embed/meE5aaRJ0Zs title="Canonical ES finds a bug in Qbert (Full)" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><p><br><br>piestyx</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://piestyx.dev/tags/%23blog/>#Blog</a></li></ul><nav class=paginav><a class=prev href=https://piestyx.dev/blog/blog-05/><span class=title>« Prev</span><br><span>Mary's Mirror</span>
</a><a class=next href=https://piestyx.dev/blog/changelog-01/><span class=title>Next »</span><br><span>Website Updated</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://piestyx.dev/>piestyx.dev</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>