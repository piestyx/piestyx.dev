<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>My AI Is A Gamer | piestyx.dev</title>
<meta name=keywords content="#devlog,#echo"><meta name=description content="Project devlog: LLM inferencing on prompt of context from the game logs, the loop is explained…"><meta name=author content="piestyx"><link rel=canonical href=https://piestyx.dev/blog/devlog-01/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://piestyx.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://piestyx.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://piestyx.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://piestyx.dev/apple-touch-icon.png><link rel=mask-icon href=https://piestyx.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://piestyx.dev/blog/devlog-01/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://piestyx.dev/css/custom.css><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><meta property="og:url" content="https://piestyx.dev/blog/devlog-01/"><meta property="og:site_name" content="piestyx.dev"><meta property="og:title" content="My AI Is A Gamer"><meta property="og:description" content="Project devlog: LLM inferencing on prompt of context from the game logs, the loop is explained…"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-08T00:00:00+00:00"><meta property="article:tag" content="#Devlog"><meta property="article:tag" content="#Echo"><meta name=twitter:card content="summary"><meta name=twitter:title content="My AI Is A Gamer"><meta name=twitter:description content="Project devlog: LLM inferencing on prompt of context from the game logs, the loop is explained…"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://piestyx.dev/blog/"},{"@type":"ListItem","position":2,"name":"My AI Is A Gamer","item":"https://piestyx.dev/blog/devlog-01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"My AI Is A Gamer","name":"My AI Is A Gamer","description":"Project devlog: LLM inferencing on prompt of context from the game logs, the loop is explained…","keywords":["#devlog","#echo"],"articleBody":"“I’ll move East to avoid that dangerous baboon” Update is actually a few days overdue, but the gameplay loop has now been tested and works end to end!\nIt’s grown more than I expected this module would, but that’s probably down to the way I’m splitting functions out to make it more ‘piestyx-logical’ and easier for me to stay on top of everything. The actual runtime flow is shown in the mermaid chart below:\ngraph TD subgraph Game_Caves_of_Qud qud_engine[\"Qud Engine\"] log_output[\"message_log.txt\"] status_json[\"status JSONs: hp, status, zone, inventory\"] end subgraph AI_Runtime watcher[\"perception.py - Log Watcher\"] buffer[\"event_buffer - context memory\"] prompt_builder[\"prompt_builder.py - build_prompts()\"] runner[\"llama_runner.py - LLMRunner\"] action_parser[\"action_parser.py - parse_action()\"] inventory_parser[\"inventory_parser.py - parse_inventory_action()\"] executor[\"executor.py - decide_and_act()\"] end subgraph LLM_Model model[\"llama-run: *chosen model*\"] end qud_engine --\u003e|Appends events| log_output qud_engine --\u003e|Exports state| status_json log_output --\u003e watcher status_json --\u003e buffer watcher --\u003e buffer buffer --\u003e prompt_builder prompt_builder --\u003e runner runner --\u003e model model --\u003e runner runner --\u003e action_parser runner --\u003e inventory_parser action_parser --\u003e executor inventory_parser --\u003e executor executor --\u003e|Inject command| qud_engineWhat’s the game saying? Everything starts with the mod that exports logs to message_log.txt, and various game states into files like hp_status.json and inventory.json. The mod uses a Postfix Harmony patch to inject export behavior into the existing MessageQueue_AddPlayerMessage class — effectively modifying the __result to include an export. Simple!\nThat output forms the foundation for the rest of the runtime, with state exports used downstream to build context for the prompt.\nListening perception.py watches the logs and strips out all the embedded tags used in-game for color and formatting. I use this cleaned version to classify each line by ’type’, making them easier to parse and format later. It also writes to qud_decision_log.json for downstream fine-tuning.\nThis is also where the prompt context is built. Since I initially used the wrong model, I added logic to format prompts for two different models, with a runtime argument flag to switch between them.\nTelling prompt_builder.py takes the structured context and formats it into prompts suitable for LLM inference. As with the context, it’s split for two models and tailored to their optimal prompt styles. I quickly found that inconsistent results often came from inconsistent system_prompts which was a side effect of logic being scattered across files. Now it’s all consolidated into prompt_builder.py, so I only need to tweak that file for changes.\nOnce the prompt is ready, it’s passed through llama.cpp into the LLM network, and out comes something like:\n`\"llm_output\": \"I would like to drink from the deep pool of salty water to quench my thirst and replenish my energy.\\u001b[0m\"` Not…ideal. Salt water doesn’t typically quench thirst, but it’s new to the game. It’ll learn.\nDoing That response is passed into action_parser.py and inventory_parser.py. Originally there was only parser.py, but I hadn’t accounted for inventory interactions. It could open the inventory, sure, just not do anything in there. Hence: inventory_parser.py.\nThe main loop now checks if the last keypress opened the inventory. If so, it parses follow-up commands to equip, use, or eat items. Afterwards, it resets inventoryOpen = False, and gameplay continues.\nFinally, executor.py sends keypresses to the game via xdotool. The game reacts, logs update, and the loop resets.\nAI gameplay is like Ogres, who are like onions…or parfait We’re talking layers, obviously.\nI didn’t fully appreciate just how many layers this module would require. On the surface it’s simple: parse game logs → prompt LLM → act out the result. But the reality is far more intricate.\nThat inventory example is a good case. I was elated seeing the AI open the inventory after an LLM response … only to realise it now had to do something inside that menu. As much as the response has been reasoned and presented back based on context, the response doesn’t mean anything to the AI.\nThese moments when solving one problem immediately presents a new one I hadn’t considered are awesome though. The challenge becomes how to solve it without breaking everything that’s already working, but even if it does break I know it won’t be that way for too long.\nIt’s taken iteration, a few required scope changes, and some unrecognised scope creep but it’s now functioning. What’s left now is refining the prompt engineering and making sure actions are parsed cleanly into the executor.\nOn to the next modules.\npiestyx\n","wordCount":"716","inLanguage":"en","datePublished":"2025-06-08T00:00:00Z","dateModified":"2025-06-08T00:00:00Z","author":{"@type":"Person","name":"piestyx"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://piestyx.dev/blog/devlog-01/"},"publisher":{"@type":"Organization","name":"piestyx.dev","logo":{"@type":"ImageObject","url":"https://piestyx.dev/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://piestyx.dev/ accesskey=h title="piestyx.dev (Alt + H)">piestyx.dev</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://piestyx.dev/packages/ title=Packages><span>Packages</span></a></li><li><a href=https://piestyx.dev/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://piestyx.dev/project_echo/ title=ECHO><span>ECHO</span></a></li><li><a href=https://piestyx.dev/playground/ title=Playground><span>Playground</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">My AI Is A Gamer</h1><div class=post-meta><span title='2025-06-08 00:00:00 +0000 UTC'>June 8, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;piestyx</div></header><div class=post-content><h2 id=ill-move-east-to-avoid-that-dangerous-baboon>&ldquo;I&rsquo;ll move East to avoid that dangerous baboon&rdquo;<a hidden class=anchor aria-hidden=true href=#ill-move-east-to-avoid-that-dangerous-baboon>#</a></h2><p>Update is actually a few days overdue, but the gameplay loop has now been tested and works end to end!</p><p>It’s grown more than I expected this module would, but that’s probably down to the way I’m splitting functions out to make it more &lsquo;piestyx-logical&rsquo; and easier for me to stay on top of everything. The actual runtime flow is shown in the mermaid chart below:</p><pre class=mermaid>graph TD
    subgraph Game_Caves_of_Qud
        qud_engine[&#34;Qud Engine&#34;]
        log_output[&#34;message_log.txt&#34;]
        status_json[&#34;status JSONs: hp, status, zone, inventory&#34;]
    end

    subgraph AI_Runtime
        watcher[&#34;perception.py - Log Watcher&#34;]
        buffer[&#34;event_buffer - context memory&#34;]
        prompt_builder[&#34;prompt_builder.py - build_prompts()&#34;]
        runner[&#34;llama_runner.py - LLMRunner&#34;]
        action_parser[&#34;action_parser.py - parse_action()&#34;]
        inventory_parser[&#34;inventory_parser.py - parse_inventory_action()&#34;]
        executor[&#34;executor.py - decide_and_act()&#34;]
    end

    subgraph LLM_Model
        model[&#34;llama-run: *chosen model*&#34;]
    end

    qud_engine --&gt;|Appends events| log_output
    qud_engine --&gt;|Exports state| status_json
    log_output --&gt; watcher
    status_json --&gt; buffer
    watcher --&gt; buffer
    buffer --&gt; prompt_builder
    prompt_builder --&gt; runner
    runner --&gt; model
    model --&gt; runner
    runner --&gt; action_parser
    runner --&gt; inventory_parser
    action_parser --&gt; executor
    inventory_parser --&gt; executor
    executor --&gt;|Inject command| qud_engine</pre><h2 id=whats-the-game-saying>What&rsquo;s the game saying?<a hidden class=anchor aria-hidden=true href=#whats-the-game-saying>#</a></h2><p>Everything starts with the mod that exports logs to <code>message_log.txt</code>, and various game states into files like <code>hp_status.json</code> and <code>inventory.json</code>. The mod uses a <code>Postfix</code> Harmony patch to inject export behavior into the existing <code>MessageQueue_AddPlayerMessage</code> class — effectively modifying the <code>__result</code> to include an export. Simple!</p><p>That output forms the foundation for the rest of the runtime, with state exports used downstream to build context for the prompt.</p><h2 id=listening>Listening<a hidden class=anchor aria-hidden=true href=#listening>#</a></h2><p><code>perception.py</code> watches the logs and strips out all the embedded tags used in-game for color and formatting. I use this cleaned version to classify each line by &rsquo;type&rsquo;, making them easier to parse and format later. It also writes to <code>qud_decision_log.json</code> for downstream fine-tuning.</p><p>This is also where the prompt context is built. Since I initially used the wrong model, I added logic to format prompts for two different models, with a runtime argument flag to switch between them.</p><h2 id=telling>Telling<a hidden class=anchor aria-hidden=true href=#telling>#</a></h2><p><code>prompt_builder.py</code> takes the structured context and formats it into prompts suitable for LLM inference. As with the context, it&rsquo;s split for two models and tailored to their optimal prompt styles. I quickly found that inconsistent results often came from inconsistent <code>system_prompts</code> which was a side effect of logic being scattered across files. Now it’s all consolidated into <code>prompt_builder.py</code>, so I only need to tweak that file for changes.</p><p>Once the prompt is ready, it’s passed through llama.cpp into the LLM network, and out comes something like:</p><pre><code>`&quot;llm_output&quot;: &quot;I would like to drink from the deep pool of salty water to quench my thirst and replenish my energy.\u001b[0m&quot;`
</code></pre><p>Not&mldr;ideal. Salt water doesn’t typically quench thirst, but it’s new to the game. It’ll learn.</p><h2 id=doing>Doing<a hidden class=anchor aria-hidden=true href=#doing>#</a></h2><p>That response is passed into <code>action_parser.py</code> and <code>inventory_parser.py</code>. Originally there was only <code>parser.py</code>, but I hadn’t accounted for inventory interactions. It could open the inventory, sure, just not do anything in there. Hence: <code>inventory_parser.py</code>.</p><p>The main loop now checks if the last keypress opened the inventory. If so, it parses follow-up commands to equip, use, or eat items. Afterwards, it resets <code>inventoryOpen = False</code>, and gameplay continues.</p><p>Finally, <code>executor.py</code> sends keypresses to the game via <a href=https://github.com/jordansissel/xdotool>xdotool</a>. The game reacts, logs update, and the loop resets.</p><h2 id=ai-gameplay-is-like-ogres-who-are-like-onionsor-parfait>AI gameplay is like Ogres, who are like onions&mldr;or parfait<a hidden class=anchor aria-hidden=true href=#ai-gameplay-is-like-ogres-who-are-like-onionsor-parfait>#</a></h2><p>We&rsquo;re talking layers, obviously.</p><p>I didn’t fully appreciate just how many layers this module would require. On the surface it’s simple: parse game logs → prompt LLM → act out the result. But the reality is far more intricate.</p><p>That inventory example is a good case. I was elated seeing the AI open the inventory after an LLM response &mldr; only to realise it now had to do something inside that menu. As much as the response has been reasoned and presented back based on context, the response doesn&rsquo;t <em>mean</em> anything to the AI.</p><p>These moments when solving one problem immediately presents a new one I hadn’t considered are awesome though. The challenge becomes how to solve it without breaking everything that’s already working, but even if it does break I know it won&rsquo;t be that way for too long.</p><p>It’s taken iteration, a few required scope changes, and some unrecognised scope creep but it’s now functioning. What&rsquo;s left now is refining the prompt engineering and making sure actions are parsed cleanly into the executor.</p><p>On to the next modules.</p><p>piestyx</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://piestyx.dev/tags/%23devlog/>#Devlog</a></li><li><a href=https://piestyx.dev/tags/%23echo/>#Echo</a></li></ul><nav class=paginav><a class=prev href=https://piestyx.dev/blog/blog-02/><span class=title>« Prev</span><br><span>AI, Heidegger, And Ontological Ambiguity</span>
</a><a class=next href=https://piestyx.dev/blog/blog-01/><span class=title>Next »</span><br><span>The Basilisk Stirs...</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://piestyx.dev/>piestyx.dev</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>