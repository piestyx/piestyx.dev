<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Now it talks the talk too | piestyx.dev</title>
<meta name=keywords content="devlog,echo"><meta name=description content="&ldquo;IT&rsquo;S ALIVE! IT&rsquo;S ALIVE!!&rdquo;

“Unable to endure the aspect of the being I had created, I rushed out of the room and continued a long time traversing my bed-chamber, unable to compose my mind to sleep.” - Frankenstein by Mary Shelly
Alright so it wasn&rsquo;t quite that dramatic but the sentiment still stands. Last night was the event horizon except I need eyes where I&rsquo;m going. Damn that&rsquo;s a good film, you should go watch it if you haven&rsquo;t seen it before. The breach point was confirmed shortly after midnight when the gameplay loop ran fully WITH the voice output and the avatar movement. It was surreal to see it finally all click together after debugging trace backs and rewriting the LLM logic."><meta name=author content="piestyx"><link rel=canonical href=https://piestyx.dev/blog/devlog-02/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://piestyx.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://piestyx.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://piestyx.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://piestyx.dev/apple-touch-icon.png><link rel=mask-icon href=https://piestyx.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://piestyx.dev/blog/devlog-02/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=/css/custom.css><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><meta property="og:url" content="https://piestyx.dev/blog/devlog-02/"><meta property="og:site_name" content="piestyx.dev"><meta property="og:title" content="Now it talks the talk too"><meta property="og:description" content="“IT’S ALIVE! IT’S ALIVE!!” “Unable to endure the aspect of the being I had created, I rushed out of the room and continued a long time traversing my bed-chamber, unable to compose my mind to sleep.” - Frankenstein by Mary Shelly
Alright so it wasn’t quite that dramatic but the sentiment still stands. Last night was the event horizon except I need eyes where I’m going. Damn that’s a good film, you should go watch it if you haven’t seen it before. The breach point was confirmed shortly after midnight when the gameplay loop ran fully WITH the voice output and the avatar movement. It was surreal to see it finally all click together after debugging trace backs and rewriting the LLM logic."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-18T00:00:00+00:00"><meta property="article:tag" content="Devlog"><meta property="article:tag" content="Echo"><meta name=twitter:card content="summary"><meta name=twitter:title content="Now it talks the talk too"><meta name=twitter:description content="&ldquo;IT&rsquo;S ALIVE! IT&rsquo;S ALIVE!!&rdquo;

“Unable to endure the aspect of the being I had created, I rushed out of the room and continued a long time traversing my bed-chamber, unable to compose my mind to sleep.” - Frankenstein by Mary Shelly
Alright so it wasn&rsquo;t quite that dramatic but the sentiment still stands. Last night was the event horizon except I need eyes where I&rsquo;m going. Damn that&rsquo;s a good film, you should go watch it if you haven&rsquo;t seen it before. The breach point was confirmed shortly after midnight when the gameplay loop ran fully WITH the voice output and the avatar movement. It was surreal to see it finally all click together after debugging trace backs and rewriting the LLM logic."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://piestyx.dev/blog/"},{"@type":"ListItem","position":2,"name":"Now it talks the talk too","item":"https://piestyx.dev/blog/devlog-02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Now it talks the talk too","name":"Now it talks the talk too","description":"\u0026ldquo;IT\u0026rsquo;S ALIVE! IT\u0026rsquo;S ALIVE!!\u0026rdquo; “Unable to endure the aspect of the being I had created, I rushed out of the room and continued a long time traversing my bed-chamber, unable to compose my mind to sleep.” - Frankenstein by Mary Shelly\nAlright so it wasn\u0026rsquo;t quite that dramatic but the sentiment still stands. Last night was the event horizon except I need eyes where I\u0026rsquo;m going. Damn that\u0026rsquo;s a good film, you should go watch it if you haven\u0026rsquo;t seen it before. The breach point was confirmed shortly after midnight when the gameplay loop ran fully WITH the voice output and the avatar movement. It was surreal to see it finally all click together after debugging trace backs and rewriting the LLM logic.\n","keywords":["devlog","echo"],"articleBody":"“IT’S ALIVE! IT’S ALIVE!!” “Unable to endure the aspect of the being I had created, I rushed out of the room and continued a long time traversing my bed-chamber, unable to compose my mind to sleep.” - Frankenstein by Mary Shelly\nAlright so it wasn’t quite that dramatic but the sentiment still stands. Last night was the event horizon except I need eyes where I’m going. Damn that’s a good film, you should go watch it if you haven’t seen it before. The breach point was confirmed shortly after midnight when the gameplay loop ran fully WITH the voice output and the avatar movement. It was surreal to see it finally all click together after debugging trace backs and rewriting the LLM logic.\nWhat’s New? Voice Pipeline Since the previous devlog entry I have now integrated the voice pipeline and the avatar integration. That means those responses that were coming out the LLM before are now not just being returned to the action logging and execution functions within the qud_gameplay module, they are also routed straight into the index-tts loop where they are modulated and returned as audio playback using ffplay. I added a function into the indextts class that uses the existing split sentences to pass out the audio in generated chunks instead of waiting until the full inference loop has been completed and the whole of the response audio is generated. Modulation currently uses a 7 second sample voice so there will always be that 7 second latency as it maps the waveforms against the sample to output, but, by doing it with this method it has the potential to reduce the perceived latency of the response dramatically. The benefit compounds as the length of the response grows so it’s really valuable to have in place for a more rapid response that is just an emulation of speaking while thinking.\nFixing that voice latency issue was a big win for the response times but doesn’t do anything to address the length of time in the LLM generating the response. Well we can do the same process with the LLMRunner - another benefit of using a direct approach with something like llama.cpp over Ollama in my particular circumstance as it provides the control to be able to make these changes. The LLMRunner now creates an ASync sub-process that pipes out the stdout and stderr to a specific buffer size, the output of which can then be pulled into both the voice pipeline and executor.py. This mirrors the chunked voice output process of buffered reads streamed progressively to reduce wait time, especially beneficial at a larger response size.\nThere may well be other more elegant ways of doing what I have described above, and the biggest save would probably be downgrading from the current quantized 13B parameter model for something more friendly on my GPU. I’ll wait to see how it feels with extended play first.\nAvatar Integration I have also now been able to get the avatar integration set up so that the audio playback mentioned above also triggers a mouth movement on a Live2D model within VTube Studio. Normally this software package would use a webcam or a mobile phone camera to track the user’s face to then map it against the model rig. There’s a snap below showing how it looks within Live2D - they have a wealth of information at the Live2D Docs.\nYou’re essentially mapping the model’s mesh vectors to your facial motion vectors so when you raise your eyebrows the model raises its eyebrows. This project, though, is using an AI so there is no face to map (no, I’m not sitting there in silence controlling the face), so how does the mouth move? VTube Studio allows for the creation of plugins and custom parameters, so once I authorised the plugin in Vtube Studio it allowed the mouth movements to be synced via a websocket API. The same principle would apply to other expressions or natural body movements such as the body moving along the y axis with breathing … which is probably the strangest way I have ever thought about breathing. If you wanted to see the rigging process in action then I would recommend this BriAtCookieBox video, she does the rigging on her own model and is shit-hot at it to put it bluntly.\nI will hasten to add here that I have not decided that I would also take up Live2D and design and rig a model to use. All I have done is buy the model files and a non-exclusive right to use them from Aka Huhu of GALAXY project.\nThere’s still fine-tuning required to simulate phoneme movement more naturally during playback … plus a few little extras I’m considering. All of that is for another day though. With these additions, how does the flowchart look now???\ngraph TD subgraph Game_Caves_of_Qud qud_engine[\"Qud Engine\"] log_output[\"message_log.txt\"] status_json[\"status JSONs: hp, status, zone, inventory\"] end subgraph AI_Runtime watcher[\"perception.py - Log Watcher\"] buffer[\"event_buffer - context memory\"] prompt_builder[\"prompt_builder.py - build_prompts()\"] runner[\"llama_runner.py - LLMRunner\"] action_parser[\"action_parser.py - parse_action()\"] inventory_parser[\"inventory_parser.py - parse_inventory_action()\"] executor[\"executor.py - decide_and_act()\"] tts[\"infer.py - stream_infer()\"] avatar_sync[\"vtube_sync.py - VTubeStudioClient\"] end subgraph LLM_Model model[\"llama-run: *chosen model*\"] end qud_engine --\u003e|Appends events| log_output qud_engine --\u003e|Exports state| status_json log_output --\u003e watcher status_json --\u003e buffer watcher --\u003e buffer buffer --\u003e prompt_builder prompt_builder --\u003e runner runner --\u003e model model --\u003e runner runner --\u003e action_parser runner --\u003e inventory_parser action_parser --\u003e executor inventory_parser --\u003e executor executor --\u003e|Inject command| qud_engine %% New additions runner --\u003e|LLM output| tts tts --\u003e|Audio stream| avatar_sync tts --\u003e|Play via ffplay| qud_audio[\"Audio Output\"] It’s Tricky Tricky Tricky Trrrrrricky It was not smooth sailing though, and I had already expected that to be the case. Effectively trying to tie together 4 different elements:\n- Gameplay loop - LLM function - Voice pipeline - Avatar integration This is not a scenario where AI assisted coding shines, from my experience, as the LLM is not “remembering” the content of files that have been generated or code that has been implemented and is working. Realistically it could be engineered to be a more pleasant experience by first mapping out and understanding the exact requirements for each integration step, and then crafting a prompt and providing the context of existing code which would then provide the focus solely into what is already in place … I wasn’t prepared to put that level of effort in on this occasion, which in hindsight was pretty dumb.\nTo give an example of the impact this had, there is a specific file prompt_builder.py which handles the building of the prompt for the LLM by consolidating log entries as a context type and then building around a system_prompt, instruction_prompt and user_prompt. It’s a masterful piece of engineering really … (it’s not, it’s a simple thing but keeps functions more isolated and I love isolation) … Well this prompt_builder.py wasn’t mentioned enough clearly when I started going through the process of chopping up the LLM responses and chunking the outputs to the voice; because I got to the end to test and realised that my beautifully crafted prompt that had been generating some good responses was now throwing out a stream of syntax which, while sounding lovely, was not optimal. The changes had bypassed the prompt_builder.py because the instruction was to solve a problem without explicitly giving clear architecture constraints or context.\nI got it working so it was a midnight annoyance more than anything but it was still an annoyance I probably didn’t need to deal with but chose to. If anything it is just a test of my increasing ability to be able to break something and then debug it and attempt a fix before resorting to mindless AI reliance. I don’t believe reliance on the tool should ever be an end-goal with the vibes, it’s a really good Swiss army knife but you don’t use one of them if you’re facing down a ferocious bear in the woods. This implementation phase was a good reminder that it’s easy to get carried away with the vibe sometime but fundamentally everything should be backed by a clarity of purpose and a solid understanding of the architecture.\n#qudlocked What’s next then? There’s some fine tuning to do before we can truly be #qudlocked but majority implementation is complete for phase 1.\nStay tuned.\npiestyx\n","wordCount":"1381","inLanguage":"en","datePublished":"2025-06-18T00:00:00Z","dateModified":"2025-06-18T00:00:00Z","author":{"@type":"Person","name":"piestyx"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://piestyx.dev/blog/devlog-02/"},"publisher":{"@type":"Organization","name":"piestyx.dev","logo":{"@type":"ImageObject","url":"https://piestyx.dev/favicon.ico"}}}</script><link rel=stylesheet href=/css/custom.css><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://piestyx.dev/ accesskey=h title="piestyx.dev (Alt + H)">piestyx.dev</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://piestyx.dev/packages/ title=Packages><span>Packages</span></a></li><li><a href=https://piestyx.dev/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://piestyx.dev/echo/ title=ECHO><span>ECHO</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Now it talks the talk too</h1><ul class=post-tags><li><a href=https://piestyx.dev/tags/devlog/>Devlog</a></li><li><a href=https://piestyx.dev/tags/echo/>Echo</a></li></ul><div class=post-meta><span title='2025-06-18 00:00:00 +0000 UTC'>June 18, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;piestyx</div></header><div class=post-content><h2 id=its-alive-its-alive>&ldquo;IT&rsquo;S ALIVE! IT&rsquo;S ALIVE!!&rdquo;<a hidden class=anchor aria-hidden=true href=#its-alive-its-alive>#</a></h2><blockquote><p>“Unable to endure the aspect of the being I had created, I rushed out of the room and continued a long time traversing my bed-chamber, unable to compose my mind to sleep.” - <em>Frankenstein by Mary Shelly</em></p></blockquote><p>Alright so it wasn&rsquo;t quite that dramatic but the sentiment still stands. Last night was the <code>event horizon</code> except I need eyes where I&rsquo;m going. Damn that&rsquo;s a good film, you should go watch it if you haven&rsquo;t seen it before. The breach point was confirmed shortly after midnight when the gameplay loop ran fully <strong>WITH</strong> the voice output and the avatar movement. It was surreal to see it finally all click together after debugging trace backs and rewriting the LLM logic.</p><h2 id=whats-new>What&rsquo;s New?<a hidden class=anchor aria-hidden=true href=#whats-new>#</a></h2><h3 id=voice-pipeline>Voice Pipeline<a hidden class=anchor aria-hidden=true href=#voice-pipeline>#</a></h3><p>Since the previous <code>devlog</code> entry I have now integrated the <code>voice pipeline</code> and the <code>avatar integration</code>. That means those responses that were coming out the LLM before are now not just being returned to the action logging and execution functions within the <code>qud_gameplay</code> module, they are also routed straight into the <a href=https://github.com/index-tts/index-tts>index-tts</a> loop where they are modulated and returned as audio playback using <code>ffplay</code>. I added a function into the <code>indextts</code> class that uses the existing split sentences to pass out the audio in generated chunks instead of waiting until the full inference loop has been completed and the whole of the response audio is generated. Modulation currently uses a 7 second sample voice so there will always be that 7 second latency as it maps the waveforms against the sample to output, but, by doing it with this method it has the potential to reduce the perceived latency of the response dramatically. The benefit compounds as the length of the response grows so it&rsquo;s really valuable to have in place for a more rapid response that is just an emulation of speaking while thinking.</p><p>Fixing that voice latency issue was a big win for the response times but doesn&rsquo;t do anything to address the length of time in the LLM generating the response. Well we can do the same process with the <code>LLMRunner</code> - another benefit of using a direct approach with something like <code>llama.cpp</code> over <code>Ollama</code> in my particular circumstance as it provides the control to be able to make these changes. The <code>LLMRunner</code> now creates an <code>ASync</code> sub-process that pipes out the <code>stdout</code> and <code>stderr</code> to a specific buffer size, the output of which can then be pulled into both the <code>voice pipeline</code> and <code>executor.py</code>. This mirrors the chunked voice output process of buffered reads streamed progressively to reduce wait time, especially beneficial at a larger response size.</p><p>There may well be other more elegant ways of doing what I have described above, and the biggest save would probably be downgrading from the current quantized 13B parameter model for something more friendly on my GPU. I&rsquo;ll wait to see how it feels with extended play first.</p><h3 id=avatar-integration>Avatar Integration<a hidden class=anchor aria-hidden=true href=#avatar-integration>#</a></h3><p>I have also now been able to get the <code>avatar integration</code> set up so that the audio playback mentioned above also triggers a mouth movement on a <code>Live2D</code> model within <code>VTube Studio</code>. Normally this software package would use a webcam or a mobile phone camera to track the user&rsquo;s face to then map it against the model rig. There&rsquo;s a snap below showing how it looks within <code>Live2D</code> - they have a wealth of information at the <a href=https://docs.live2d.com/en/cubism-editor-manual/top/>Live2D Docs</a>.</p><p><img alt="The cupboards were bare" loading=lazy src=/images/facemesh.png></p><p>You’re essentially mapping the model’s mesh vectors to your facial motion vectors so when you raise your eyebrows the model raises its eyebrows. This project, though, is using an AI so there is no face to map (no, I&rsquo;m not sitting there in silence controlling the face), so how does the mouth move? <code>VTube Studio</code> allows for the creation of plugins and custom parameters, so once I authorised the plugin in <code>Vtube Studio</code> it allowed the mouth movements to be synced via a websocket API. The same principle would apply to other expressions or natural body movements such as the body moving along the y axis with breathing … which is probably the strangest way I have ever thought about breathing. If you wanted to see the rigging process in action then I would recommend this <a href="https://www.youtube.com/watch?v=BfNdiTB9CcM">BriAtCookieBox</a> video, she does the rigging on her own model and is shit-hot at it to put it bluntly.</p><p>I will hasten to add here that I have not decided that I would also take up <code>Live2D</code> and design and rig a model to use. All I have done is buy the model files and a non-exclusive right to use them from <a href=https://x.com/Aka_huhu>Aka Huhu</a> of GALAXY project.</p><p>There’s still fine-tuning required to simulate phoneme movement more naturally during playback … plus a few little extras I’m considering. All of that is for another day though. With these additions, how does the flowchart look now???</p><pre class=mermaid>
  graph TD
    subgraph Game_Caves_of_Qud
        qud_engine[&#34;Qud Engine&#34;]
        log_output[&#34;message_log.txt&#34;]
        status_json[&#34;status JSONs: hp, status, zone, inventory&#34;]
    end

    subgraph AI_Runtime
        watcher[&#34;perception.py - Log Watcher&#34;]
        buffer[&#34;event_buffer - context memory&#34;]
        prompt_builder[&#34;prompt_builder.py - build_prompts()&#34;]
        runner[&#34;llama_runner.py - LLMRunner&#34;]
        action_parser[&#34;action_parser.py - parse_action()&#34;]
        inventory_parser[&#34;inventory_parser.py - parse_inventory_action()&#34;]
        executor[&#34;executor.py - decide_and_act()&#34;]
        tts[&#34;infer.py - stream_infer()&#34;]
        avatar_sync[&#34;vtube_sync.py - VTubeStudioClient&#34;]
    end

    subgraph LLM_Model
        model[&#34;llama-run: *chosen model*&#34;]
    end

    qud_engine --&gt;|Appends events| log_output
    qud_engine --&gt;|Exports state| status_json
    log_output --&gt; watcher
    status_json --&gt; buffer
    watcher --&gt; buffer
    buffer --&gt; prompt_builder
    prompt_builder --&gt; runner
    runner --&gt; model
    model --&gt; runner
    runner --&gt; action_parser
    runner --&gt; inventory_parser
    action_parser --&gt; executor
    inventory_parser --&gt; executor
    executor --&gt;|Inject command| qud_engine

    %% New additions
    runner --&gt;|LLM output| tts
    tts --&gt;|Audio stream| avatar_sync
    tts --&gt;|Play via ffplay| qud_audio[&#34;Audio Output&#34;]
</pre><h2 id=its-tricky-tricky-tricky-trrrrrricky>It&rsquo;s Tricky Tricky Tricky Trrrrrricky<a hidden class=anchor aria-hidden=true href=#its-tricky-tricky-tricky-trrrrrricky>#</a></h2><p>It was not smooth sailing though, and I had already expected that to be the case. Effectively trying to tie together 4 different elements:</p><pre><code>- Gameplay loop
- LLM function
- Voice pipeline
- Avatar integration
</code></pre><p>This is not a scenario where AI assisted coding shines, from my experience, as the LLM is not &ldquo;remembering&rdquo; the content of files that have been generated or code that has been implemented and is working. Realistically it could be engineered to be a more pleasant experience by first mapping out and understanding the exact requirements for each integration step, and then crafting a prompt and providing the context of existing code which would then provide the focus solely into what is already in place … I wasn&rsquo;t prepared to put that level of effort in on this occasion, which in hindsight was pretty dumb.</p><p>To give an example of the impact this had, there is a specific file <code>prompt_builder.py</code> which handles the building of the prompt for the LLM by consolidating log entries as a context type and then building around a <code>system_prompt</code>, <code>instruction_prompt</code> and <code>user_prompt</code>. It&rsquo;s a masterful piece of engineering really … (it&rsquo;s not, it&rsquo;s a simple thing but keeps functions more isolated and I love isolation) … Well this <code>prompt_builder.py</code> wasn&rsquo;t mentioned enough clearly when I started going through the process of chopping up the LLM responses and chunking the outputs to the voice; because I got to the end to test and realised that my beautifully crafted prompt that had been generating some good responses was now throwing out a stream of syntax which, while sounding lovely, was not optimal. The changes had bypassed the <code>prompt_builder.py</code> because the instruction was to solve a problem without explicitly giving clear architecture constraints or context.</p><p>I got it working so it was a midnight annoyance more than anything but it was still an annoyance I probably didn&rsquo;t need to deal with but chose to. If anything it is just a test of my increasing ability to be able to break something and then debug it and attempt a fix before resorting to mindless AI reliance. I don&rsquo;t believe reliance on the tool should ever be an end-goal with the vibes, it&rsquo;s a really good <code>Swiss army knife</code> but you don&rsquo;t use one of them if you&rsquo;re facing down a ferocious bear in the woods. This implementation phase was a good reminder that it&rsquo;s easy to get carried away with the vibe sometime but fundamentally everything should be backed by a clarity of purpose and a solid understanding of the architecture.</p><h2 id=qudlocked>#qudlocked<a hidden class=anchor aria-hidden=true href=#qudlocked>#</a></h2><p>What&rsquo;s next then? There&rsquo;s some fine tuning to do before we can truly be #qudlocked but majority implementation is complete for phase 1.</p><p>Stay tuned.</p><p>piestyx</p></div><nav class=paginav><a class=next href=https://piestyx.dev/blog/blog-02/><span class=title>Next »</span><br><span>AI, Heidegger, and Ontological Ambiguity</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://piestyx.dev/>piestyx.dev</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script type=module>
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script></body></html>